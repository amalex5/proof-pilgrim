<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Everything Is Polynomic: Adventures With Taylor Series</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="/main.css" />
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <script src="../highlight-toc.js" type="text/javascript"></script>

  <script> 
      // disappearing header, from dynomight.net'
      var prevScrollpos = window.pageYOffset;

      window.onscroll = function() {
        var currentScrollPos = Math.max(0,window.pageYOffset);

        if (Math.abs(currentScrollPos - prevScrollpos) > 10){
          if (prevScrollpos > currentScrollPos) {
            document.querySelector('header').style.top = "0";
            document.querySelector('header').style.opacity = "75%";
          } else {
            document.querySelector('header').style.top = "-60px";
          }
          prevScrollpos = currentScrollPos;
        }
        if(currentScrollPos==0){
          document.querySelector('header').style.top = "0";
          document.querySelector('header').style.opacity = "100%";
        }
      }
    </script>

</head>
<body>
  <header>
  <span id="header-name"> <a href='/'>the natural history <span style="font-weight:normal;">of abstract objects</span></a></span> <span id="linkback"><a href=http://www.andrusia.com><img src="/andrew-logo.svg" id='header-logo' style='width: auto;'/></a></span>
</header>
<script>document.addEventListener('DOMContentLoaded', function(e) {
  Scroller.init();
})</script>
<!-- <aside>
<div class="tocLinks" >
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#save-yourself-the-trouble">Save Yourself The Trouble</a></li>
<li><a href="#antidifferentiation-awesomeness">Antidifferentiation Awesomeness</a></li>
<li><a href="#whence-taylor-series-the-proof">Whence Taylor Series: The Proof</a></li>
<li><a href="#problems">Problems</a></li>
</ul>
</nav>
</div>
</aside>
 -->
<main>
<div id="title-block">
<h1 class="title">Everything Is Polynomic: Adventures With Taylor Series</h1>
</div>
<blockquote>
<p><em>We shall not cease from exploration,</em> <br> <em>And the end of all our exploring,</em> <br> <em>Will be to arrive where we started</em> <br> <em>And know the place for the first time.</em> <br> <span style="text-align:right;">—T.S. Eliot, <em>Little Gidding</em></span></p>
</blockquote>
<div style="height:5em">

</div>
<p>The English philosopher/mathematician Alfred North Whitehead<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> once remarked that “all of Western philosophy is a series of footnotes to Plato.”<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> My feelings about Taylor series are similar, but the chronology is reversed. In a sense, all of the math we’ve done up until now—all of the complicated proofs and convoluted arguments—we could have done much more simply, and much more cleanly, using Taylor series. If you glance at the problems in the back, you should get an idea of their wide-ranging versatility and utility. If the world is made only of polynomials, it really is a much simpler place.</p>
<p>Given some function <span class="math inline">\(f(x)\)</span> (of which we can take derivatives and such), and some <span class="math inline">\(x\)</span>-value of the function <span class="math inline">\(a\)</span>, then we can write <span class="math inline">\(f(x)\)</span> as a (possibly-infinitely-long) polynomial in this way: <span class="math display">\[f(x) = f(a) + f&#39;(a)(x-a) + \frac{f&#39;&#39;(a)}{2!}(x-a)^2 + \frac{f&#39;&#39;&#39;(a)}{3!}(x-a)^3 + \frac{f&#39;&#39;&#39;&#39;(a)}{4!}(x-a)^4 + \cdots\]</span> or, written in <span class="math inline">\(\Sigma\)</span>-form: <span class="math display">\[f(x) = \sum_{k=0}^{\infty}\frac{f^{(k)}(a)}{k!}(x-a)^k\]</span> This is called the <strong>Taylor series</strong> or <strong>Taylor expansion</strong> of <span class="math inline">\(f(x)\)</span>. Sometimes it is also called the Taylor series/expansion <strong>around <span class="math inline">\(a\)</span></strong> or <strong>in terms of <span class="math inline">\(x-a\)</span></strong>. Taylor series where <span class="math inline">\(a=0\)</span> (which is a common choice for <span class="math inline">\(a\)</span>, since it tends to make the algebra easier) are often called <strong>Maclaurin series</strong>. If we approximate a function using a Taylor series of finite length—i.e., if we have a function that should be an infinitely-long polynomial, but we approximate it with a finitely-long one—we call that the <strong>Taylor approximation</strong> of <span class="math inline">\(f(x)\)</span>, or the <strong>Taylor polynomial of order <span class="math inline">\(n\)</span></strong>, where <span class="math inline">\(n\)</span> is the number of terms in the Taylor series we’ve written out.</p>
<p>(Note that in these notes, when I write something like <span class="math inline">\(f^{(2)}(x)\)</span> (or <span class="math inline">\(f^{(n)}(x)\)</span>), I mean the second (or <span class="math inline">\(n\)</span>th) derivative of <span class="math inline">\(f(x)\)</span>.)</p>
<p>Let’s use this to try writing <span class="math inline">\(\sin x\)</span> as a polynomial. For <span class="math inline">\(a\)</span>, let’s choose <span class="math inline">\(a=0\)</span>. (The choice of <span class="math inline">\(a\)</span> doesn’t matter; it could be any number. I’m choosing <span class="math inline">\(0\)</span> because we’ll have to plug <span class="math inline">\(a\)</span> into the various derivatives of <span class="math inline">\(\sin x\)</span>—i.e., we’ll have to plug <span class="math inline">\(a\)</span> into $x $ and <span class="math inline">\(\cos x\)</span>—and it’s a lot easier to find <span class="math inline">\(\sin(0)\)</span> and <span class="math inline">\(\cos(0)\)</span> than it is to find, say, <span class="math inline">\(\sin(43\pi/13.2)\)</span> (were I to choose <span class="math inline">\(a=43\pi/13.2\)</span>.) The formula is a tad messy, so to make things a bit clearer, let’s build it term-by-term. We’ve got a lot of derivatives to take and plug things into and compile, so I’m going to show things step-by-step in a table. In this table, I’ve listed the term index (on the left-hand column), the relevant derivative, the derivative with <span class="math inline">\(a\)</span> plugged in, and finally, the term itself.</p>
<p>So then, if I just add together all the terms in the rightmost column, I’ll get the Taylor series for <span class="math inline">\(\sin x\)</span>! I’ll have: <span class="math display">\[\sin(x) = 0 + x+0- \frac{1}{3!}x^3 +0 + \frac{1}{5!}x^5 + 0 -\frac{1}{7!}x^7 + 0 +\frac{1}{9!}x^9 - \cdots\]</span> or just: <span class="math display">\[\sin(x) = x- \frac{1}{3!}x^3 + \frac{1}{5!}x^5 -\frac{1}{7!}x^7 +\frac{1}{9!}x^9 - \cdots\]</span> There it is! We can write <span class="math inline">\(\sin(x)\)</span> as this infinitely-long polynomial, with alternating adding and substracting odd polynomials. If we like, we can represent in <span class="math inline">\(\Sigma\)</span>-form: <span class="math display">\[\sin(x) = \sum_{k=0}^{\infty} \frac{(-1)^{n+1}}{(2n+1)!}x^{2n+1}\]</span></p>
<h2 id="save-yourself-the-trouble">Save Yourself The Trouble</h2>
What if I want to write the Taylor series of, say, <span class="math inline">\(x^{20}\sin(x)\)</span>? The obvious way to do it would be to plug things into Taylor’s formula. But that will get real nasty real fast, since we’ll have to do the product rule (to find the first derivative), and then to find the second derivative we’ll have to do the product rule twice (because we get two terms), and then for the third derivative we’ll have to do the product rule four times, and so on. Once we’re past the 20th derivative the <span class="math inline">\(x^{20}\)</span> remnants will start to go away and we’ll be fine, but until then, it’ll be a huge mess.
<p>Why go to all that work? We know how to write <span class="math inline">\(\sin(x)\)</span> as a series—why don’t we just multiply the whole thing by <span class="math inline">\(x^{20}\)</span>? <span class="math display">\[\begin{align*}
x^{20}\sin(x) &amp;=x^{20}\cdot \left(x - \frac{1}{3!}x^3 + \frac{1}{5!}x^5 - \frac{1}{7!}x^7 + \frac{1}{9!}x^9 + \cdots \right) \\
 &amp;= x^{21} - \frac{1}{3!}x^{23} + \frac{1}{5!}x^{25} - \frac{1}{7!}x^{27} + \frac{1}{9!}x^{29} + \cdots \\
&amp;= \sum_{k=0}^{\infty} \frac{(-1)^{n+1}}{(2n+1)!}x^{2n+21} 
\end{align*}\]</span></p>
<p>Likewise, what if you want to find the Taylor series of <span class="math inline">\(\sin(5x)\)</span>? Again, you {} brute-force it—you could plug this into Taylor’s formula. And, actually, it’d be a lot easier than in the previous example—you’d just get a bunch of <span class="math inline">\(5\)</span>’s piling up as a result of the chain rule. But still. Here’s my idea: you already know the Taylor series of <span class="math inline">\(\sin(x)\)</span>. So why not just replace <span class="math inline">\(x\)</span> with <span class="math inline">\(5x\)</span>? We’d have: <span class="math display">\[\begin{align*}
\sin(5x) &amp;= 5x - \frac{1}{3!}(5x)^3 + \frac{1}{5!}(5x)^5 - \frac{1}{7!}(5x)^7 + \frac{1}{9!}(5x)^9 + \cdots \\
&amp;= 5x - \frac{5^3}{3!}x^3 + \frac{5^5}{5!}x^5 - \frac{5^7}{7!}x^7 + \frac{5^9}{9!}x^9 + \cdots \\  
&amp;= \sum_{k=0}^{\infty} \frac{(-1)^{n+1}5^{2n+1}}{(2n+1)!}x^{2n+1} 
\end{align*}\]</span></p>
<h2 id="antidifferentiation-awesomeness">Antidifferentiation Awesomeness</h2>
<p>Here’s a cool application of Taylor series. So, antiderivatives are a pain, right? And sometimes they’re not just hard; they’re {}. Like, literally, you can prove that it’s impossible to antidifferentiate some functions. Classic examples: <span class="math display">\[\int \! \frac{\sin x}{x} \,dx \quad\text{ and }\quad \int \!  \sqrt{1+x^4}\,dx\]</span> But really… these are only impossible to antidifferentiate {}. Here’s what I mean: We know we can write sine using a Taylor series: <span class="math display">\[\sin(x) = x - \frac{1}{3!}x^3 + \frac{1}{5!}x^5 - \frac{1}{7!}x^7 + \frac{1}{9!}x^9 + \cdots\]</span> So then we should be able to write <span class="math inline">\(\sin(x)/x\)</span> like this: <span class="math display">\[\begin{align*}
\frac{\sin(x)}{x} &amp;= \left(\frac{1}{x}\right)\sin(x) \\
 &amp;= \left(\frac{1}{x}\right)\cdot\left(x - \frac{1}{3!}x^3 + \frac{1}{5!}x^5 - \frac{1}{7!}x^7 + \frac{1}{9!}x^9 + \cdots \right) \\
 \intertext{}
 &amp;= 1 - \frac{1}{3!}x^2 + \frac{1}{5!}x^4 - \frac{1}{7!}x^6 + \frac{1}{9!}x^8 + \cdots \\
&amp;= \sum_{k=0}^{\infty} \frac{(-1)^{n+1}}{(2n+1)!}x^{2n} 
\end{align*}\]</span> But wait… this is just a polynomial. We can antidifferentiate polynomails. That’s {}. We must just have: <span class="math display">\[\begin{align*}
\int \! \frac{\sin x}{x}\,dx &amp;= \int \! 1 - \frac{1}{3!}x^2 + \frac{1}{5!}x^4 - \frac{1}{7!}x^6 + \frac{1}{9!}x^8 +\cdots \,dx \\
&amp;= x - \frac{1}{3!\cdot 3}x^3 + \frac{1}{5!\cdot5}x^5 - \frac{1}{7!\cdot 7}x^7 + \frac{1}{9!\cdot 9}x^9 +\cdots + C \\
&amp;= \sum_{k=0}^{\infty} \frac{(-1)^{n+1}}{(2n+1)!\cdot(2n+1)}x^{2n+1} 
\end{align*}\]</span> WOW!!!!! We can take an antiderivative of this supposedly-impossible-to-antidifferentiate function. That’s really, really cool.</p>
<h2 id="whence-taylor-series-the-proof">Whence Taylor Series: The Proof</h2>
<p>We should probably understand why Taylor series exist before we go much further. The basic idea is that a Taylor series is really just a tricked-out version of integration by parts. When we were doing integration by parts, we discussed how to quickly do multiple integrations-by-parts (in the case of, e.g., <span class="math inline">\(\int x^5\sin(x)dx\)</span>), and I probably made some analogy to the effect of, ``If integration by parts is a machine gun, using a table is an {}!!!’’ Taylor series continues this analogy: it makes integrating by parts or using a table seem like small-arms fire. Taylor series, by comparison, is an {}.</p>
<p>Imagine we have some function <span class="math inline">\(h(x)\)</span> and some point <span class="math inline">\(a\)</span>. Then, as we’ve discussed, we can write it as: <span class="math display">\[h(x)= h(a) + (x-a)h&#39;(a)  + \frac{1}{2!}(x-a)^2h&#39;&#39;(a) + \frac{1}{3!}(x-a)^3h&#39;&#39;&#39;(a)  +\frac{1}{4!}(x-a)^4h&#39;&#39;&#39;&#39;(a) + \cdots\]</span> (Note that I’m calling this function <span class="math inline">\(h(x)\)</span> and not <span class="math inline">\(f(x)\)</span>; that’s because in the proof I want to use the integration-by-parts formula, which already has a function labelled as <span class="math inline">\(f(x)\)</span>, and I don’t want to get it confused. Also, I wrote it in a slightly-switched around order, but multiplication doesn’t care about order (<span class="math inline">\(ab=ba\)</span>), so that’s no big deal.) Let’s prove this.</p>
<p>Where should we start? How about at the beginning—with the fundamentals? Part II of the Fundamental Theorem of Calculus says that for any function <span class="math inline">\(h(x)\)</span>, we must have: <span class="math display">\[\int_a^x \! h&#39;(t) \, dt = h(x) - h(a)\]</span> Of course, I can rearrange this so that it just looks like: <span class="math display">\[h(x) = h(a) + \int_a^x \! h&#39;(t) \, dt\]</span> And we’re on our way! We’ve already started creating a Taylor polynomial—<span class="math inline">\(h(a)\)</span> is the first term. Now I want to start hitting up <span class="math inline">\(\int h&#39;(t)dt\)</span> with integration by parts. Remember that the integration by parts formula looks like this: <span class="math display">\[\int\! f(x)g&#39;(x)\,dx = f(x)g(x) - \int\! f&#39;(x)g(x)\,dx\]</span> Of course, in our Taylor formula so far, we have an integral that only has one part. It only has <span class="math inline">\(h&#39;(t)\)</span>. But really it has {} parts—because it’s <span class="math inline">\(h&#39;(t)\)</span> multiplied by <span class="math inline">\(1\)</span>: <span class="math display">\[h(x) = h(a) + \int_a^x \! 1\cdot h&#39;(t) \, dt\]</span> So let’s choose <span class="math inline">\(1\)</span> to be our <span class="math inline">\(g&#39;(x)\)</span> (or, well, <span class="math inline">\(g&#39;(t)\)</span>), and <span class="math inline">\(h&#39;(t)\)</span> to be our <span class="math inline">\(f(t)\)</span>: <span class="math display">\[h(x) = h(a) + \int_a^x \! \underbrace{1}_{\mathclap{g&#39;(t)}}\cdot \underbrace{h&#39;(t)}_{f(t)} \, dt\]</span> And then we can list the various stuff we need to integrate by parts. (One of these is kind of weird and maybe surprising. I’ll explain in a moment.)</p>
<p>The <span class="math inline">\(f\)</span> and <span class="math inline">\(f&#39;\)</span> should make sense—<span class="math inline">\(f\)</span> is <span class="math inline">\(h&#39;\)</span>, so <span class="math inline">\(f&#39;\)</span> is <span class="math inline">\(h&#39;&#39;\)</span>. But what you should be immediately suspicious of is my choice of <span class="math inline">\(g(t)\)</span>. <span class="math inline">\(g&#39;(t)\)</span> was <span class="math inline">\(1\)</span>, so shouldn’t <span class="math inline">\(g(t)\)</span>—which is just the antiderivative of <span class="math inline">\(g&#39;(t)\)</span>—be <span class="math inline">\(t\)</span>?</p>
Yes and no. Allow me, first, to prove to you that, in fact, <span class="math inline">\(-(x-t)\)</span> {} an antiderivative of <span class="math inline">\(1\)</span>. The key step here is realizing that we’re taking the derivative/antiderivative {}, and that <span class="math inline">\(x\)</span> is {} with respect to <span class="math inline">\(t\)</span>. If I take the derivative of <span class="math inline">\(-(x-t)\)</span>, I get: <span class="math display">\[\begin{align*}
\frac{d}{dt}\left[ -(x-t)\right] &amp;= \frac{d}{dt}\left[ -x + t \right] \\
&amp; = \frac{d}{dt}\left[ -x \right]  + \frac{d}{dt}\left[t \right] \\
&amp;= 0 + 1 \\
&amp;= 1
\end{align*}\]</span> Does this make sense? The derivative of <span class="math inline">\(-(x-t)\)</span> is <span class="math inline">\(1\)</span>; therefore, the antiderivative of <span class="math inline">\(1\)</span> is <span class="math inline">\(-(x-t)\)</span>. So if I put this stuff together and integrate by parts, I get: <span class="math display">\[h(x) = h(a) + \left[-(x-t)h&#39;(t) - \int\! -(x-t)h&#39;&#39;(t)\,dt \right]_a^x\]</span> <span class="math display">\[h(x) = h(a) + \left[-(x-t)h&#39;(t)\right]_a^x + \left[ - \int\! -(x-t)h&#39;&#39;(t)\,dt \right]_a^x\]</span> And then if I evaluate that (i.e., plug in the <span class="math inline">\(x\)</span> and <span class="math inline">\(a\)</span>): <span class="math display">\[h(x) = h(a) + (-(x-x)h&#39;(x) - [-(x-a)h&#39;(a)] ) - \int_a^x\! -(x-t)h&#39;&#39;(t)\,dt \]</span> We’ve got a whole bunch of nested negatives—let’s distribute them and clean this up. <span class="math display">\[h(x) = h(a) -(\underbrace{x-x}_{=0})h&#39;(x)+ (x-a)h&#39;(a)  + \int_a^x\! (x-t)h&#39;&#39;(t)\,dt \]</span> Also, that second term has <span class="math inline">\((x-x)\)</span>, which is just zero, and so the entire term goes away: <span class="math display">\[h(x) = h(a) + 0+ (x-a)h&#39;(a)  + \int_a^x\! (x-t)h&#39;&#39;(t)\,dt \]</span> <span class="math display">\[h(x) = h(a) + (x-a)h&#39;(a)  + \int_a^x\! (x-t)h&#39;&#39;(t)\,dt \]</span> It looks like the first two terms of a Taylor series! Let’s keep going! Let’s integrate THIS by parts! <span class="math display">\[h(x) = h(a) + (x-a)h&#39;(a)  + \int_a^x\! \underbrace{(x-t)}_{g&#39;(t)}\cdot\underbrace{h&#39;&#39;(t)}_{f(t)}\,dt \]</span>
so we have: <span class="math display">\[\begin{align*}
h(x) &amp; = h(a) + (x-a)h&#39;(a)  + \left[ -\frac{1}{2}(x-t)^2h&#39;&#39;(t) - \int\! -\frac{1}{2}(x-t)^2h&#39;&#39;&#39;(t) \,dt   \right]_a^x \\ \\
&amp;= h(a) + (x-a)h&#39;(a)  + \left( -\frac{1}{2}(x-x)^2h&#39;&#39;(x) - (-\frac{1}{2}(x-a)^2h&#39;&#39;(a))\right) - \int_a^x\! -\frac{1}{2}(x-t)^2h&#39;&#39;&#39;(t) \,dt   \\ \\ 
&amp; = h(a) + (x-a)h&#39;(a)  + \left( -\frac{1}{2}(\underbrace{x-x}_{=0})^2h&#39;&#39;(x) - (-\frac{1}{2}(x-a)^2h&#39;&#39;(a))\right) - \int_a^x\! -\frac{1}{2}(x-t)^2h&#39;&#39;&#39;(t) \,dt   \\ \\
&amp; = h(a) + (x-a)h&#39;(a)  + \left( 0 - (-\frac{1}{2}(x-a)^2h&#39;&#39;(a))\right) - \int_a^x\! -\frac{1}{2}(x-t)^2h&#39;&#39;&#39;(t) \,dt   \\ \\
&amp;= h(a) + (x-a)h&#39;(a)  + \frac{1}{2}(x-a)^2h&#39;&#39;(a) - \int_a^x\! -\frac{1}{2}(x-t)^2h&#39;&#39;&#39;(t) \,dt   \\ \\
&amp;= h(a) + (x-a)h&#39;(a)  + \frac{1}{2}(x-a)^2h&#39;&#39;(a) + \int_a^x\! \frac{1}{2}(x-t)^2h&#39;&#39;&#39;(t) \,dt 
\end{align*}\]</span> I guess one of the annoying things about this proof is all the nested negatives—negatives of negatives of negatives. So be sure you keep track of those. Anyway, roll on! Let’s integrate the NEXT integral by parts!!! <span class="math display">\[h(x)= h(a) + (x-a)h&#39;(a)  + \frac{1}{2}(x-a)^2h&#39;&#39;(a) + \int_a^x\! \underbrace{\frac{1}{2}(x-t)^2}_{g&#39;(t)}\cdot\underbrace{h&#39;&#39;&#39;(t)}_{f(t)} \,dt \]</span>
<p><span class="math display">\[h(x)= h(a) + (x-a)h&#39;(a)  + \frac{1}{2}(x-a)^2h&#39;&#39;(a) +\left[ -\frac{1}{2\cdot3}(x-t)^3h&#39;&#39;&#39;(t) -  \int \! -\frac{1}{2\cdot3}(x-t)^3f&#39;(t) = h&#39;&#39;&#39;&#39;(t) \right]_a^x \]</span> I’m sort of running out of space on the page, but you get the idea: I plug in <span class="math inline">\(x\)</span> and <span class="math inline">\(a\)</span>, I get an <span class="math inline">\((x-x)\)</span> term that goes away, I blast through a bunch of negatives, and end up with something like: <span class="math display">\[h(x)= h(a) + (x-a)h&#39;(a)  + \frac{1}{2}(x-a)^2h&#39;&#39;(a) + \frac{1}{2\cdot3}(x-a)^3h&#39;&#39;&#39;(a) -  \int_a^x \! -\frac{1}{2\cdot3}(x-t)^3h&#39;&#39;&#39;&#39;(t)\,dt \]</span> So, as you can see, a Taylor polynomial is forming! We are growing a Taylor series, bit by bit. Instead of giving it sun and water, we are giving it integration by parts. All we do is integrate by parts forever, and we get a Taylor series! So this shows you where the formula comes from, which as far as I am concerned is sufficient proof. </p>
<div class="callout-box">
<div style="font-size: 150%; font-weight: bold">
Taylor Series Quick Reference!
</div>
<p><span class="math display">\[\begin{align*} 
\displaystyle \large\sin(x) &amp;= \hfill \\ \\
\displaystyle \large\cos(x) &amp;= \\ \\
\displaystyle \large e^x &amp;= \\ \\
\displaystyle \large\ln(x) &amp;= \\ \\
\displaystyle \large(1+x)^p &amp;= \\ \\
\end{align*}\]</span></p>
</div>
<h2 id="problems">Problems</h2>
<p>Use Taylor’s formula (the formula) to find the Taylor series of the following functions. (Write them out to at least the 5th nonzero term and also express them in <span class="math inline">\(\Sigma\)</span> form. If I don’t give you a value for <span class="math inline">\(a\)</span>, choose one yourself.)</p>
<ol class="problems">
<li>
<span class="math inline">\(f(x) = \cos(x)\)</span>, with <span class="math inline">\(a=0\)</span>
</li>
<li>
<span class="math inline">\(f(x) = \sqrt{x}\)</span>, around <span class="math inline">\(a=2\)</span>
</li>
<li>
<span class="math inline">\(f(x) = e^x\)</span>, with <span class="math inline">\(a=0\)</span>
</li>
<li>
<span class="math inline">\(f(x) = \ln(x)\)</span>, around <span class="math inline">\(x=1\)</span>
</li>
<li>
<span class="math inline">\(f(x) = (1+x)^p\)</span>, with <span class="math inline">\(a=0\)</span>
</li>
<li>
<span class="math inline">\(f(x) = e^{2x}\)</span>
</li>
<li>
<span class="math inline">\(f(x) = \sin(2x)\)</span>
</li>
<li>
<span class="math inline">\(f(x) = \ln(1+x)\)</span>
</li>
<li>
<span class="math inline">\(f(x) = e^x + x + \sin(x)\)</span>
</li>
<li>
<span class="math inline">\(f(x) = e^{-3x}\)</span>
</li>
<li>
<span class="math inline">\(f(x) = \sqrt{x}\)</span>, around <span class="math inline">\(a=4\)</span>
</li>
<li>
<span class="math inline">\(f(x) = \sqrt{1+x}\)</span>
</li>
<li>
<span class="math inline">\(\displaystyle f(x) = \frac{1}{(1+x)^2}\)</span>
</li>
<li>
<span class="math inline">\(f(x) = \cos(x)\)</span>, with <span class="math inline">\(a=\pi\)</span>
</li>
<li>
<span class="math inline">\(f(x) = \cos(\pi x)\)</span>, with <span class="math inline">\(a=1/2\)</span>
</li>
<li>
<span class="math inline">\(f(x) = \sin(x)\)</span>, with <span class="math inline">\(a=\pi\)</span>
</li>
<li>
<span class="math inline">\(f(x) = \sin(x)\)</span>, with <span class="math inline">\(a=\pi/6\)</span>
</li>
<li>
<span class="math inline">\(f(x) = e^x\)</span>, with <span class="math inline">\(a=1\)</span>
</li>
<li>
<span class="math inline">\(f(x) = e^{x^2}\)</span>
</li>
<li>
<span class="math inline">\(f(x) = x^2\)</span>, with <span class="math inline">\(a=0\)</span>
</li>
<li>
<span class="math inline">\(f(x) = x^2\)</span>, with <span class="math inline">\(a=1\)</span>
</li>
<li>
<span class="math inline">\(f(x) = 1 + x^2 + x^3\)</span>
</li>
<li>
<span class="math inline">\(f(x) = 2 - x + 3x^2 - x^3\)</span>, <span class="math inline">\(a=-1\)</span>
</li>
<li>
<span class="math inline">\(f(x) = 2x^5 + x^2 - 3x - 5\)</span>, with <span class="math inline">\(a=1\)</span>
</li>
<li>
<span class="math inline">\(f(x) = x^{-1}\)</span>, around <span class="math inline">\(x=-1\)</span>
</li>
<li>
<span class="math inline">\(\displaystyle f(x) = \frac{x}{(1+x)^2}\)</span>
</li>
<li>
<span class="math inline">\(\displaystyle f(x) = \frac{x+5}{(1+x)^2}\)</span>
</li>
<li>
<span class="math inline">\(f(x) = e^{kx}\)</span>
</li>
<li>
<span class="math inline">\(f(x) = \cos(kx)\)</span>
</li>
<li>
<span class="math inline">\(f(x) = x\sin(x)\)</span>
</li>
<li>
<span class="math inline">\(f(x) = xe^x\)</span>
</li>
<li>
<span class="math inline">\(f(x) = 2xe^{x^2}\)</span>
</li>
<li>
<span class="math inline">\(f(x) = x^2\cos(x^3)\)</span>
</li>
</ol>
<ol class="problems">
<li>
We’ve already seen that we can use our geometric series theorem to write the Taylor series of <span class="math inline">\(1/(1+x)\)</span>. That is to say, we’ve seen: <span class="math display">\[ \frac{1}{1-x}= \sum_{k=0}^{\infty} x^n =1+x+x^2+x^3+x^4+\cdots \]</span> Show that you get the same result using Taylor’s formula (i.e., the formula for a Taylor series). That is to say, use the function <span class="math inline">\(f(x) = 1/(1-x)\)</span> and see what the formula gives you as its Taylor series.
</li>
<li>
Find the Taylor series of <span class="math inline">\(\displaystyle f(x) = \frac{1}{1+x}\)</span> How does this relate to the Taylor series expalnsion of <span class="math inline">\(\ln(1+x)\)</span>? (How does <span class="math inline">\(\ln(1+x)\)</span> relate to <span class="math inline">\(\frac{1}{1+x}\)</span>?)
</li>
</ol>
Find the Taylor series of each of the following rational functions:
<ol class="problems">
<span class="math inline">\(\displaystyle f(x) = \frac{1}{1-x^2}\)</span>
<span class="math inline">\(\displaystyle f(x) = \frac{1}{1-x^3}\)</span>
<span class="math inline">\(\displaystyle f(x) = \frac{1}{(1-x)^2}\)</span> (ooh)
</ol>
<div style="height:3em;">

</div>
<p>In 1974, the philosopher Thomas Nagel published a seminal essay on the philosophy of mind entitled “What is it like to be a bat?<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> His thesis, simply put, was that we cannot know what it is like to be a bat, because if we did, we would either be a strange bat-human hybrid (and thus not really a bat) or a bat (and thus we wouldn’t be ourselves). In the following problems, I would like you to ask yourselves:”What is it like to be a calculator?"</p>
<ol class="problems">
<li>
Come up with an approximation (to, say, three decimal places) of <span class="math inline">\(\sin(\pi/60)\)</span>. (Suggestion: you know how to write <span class="math inline">\(\sin(x)\)</span>… what if <span class="math inline">\(x\)</span> is a specific number and not just ``<span class="math inline">\(x\)</span>’’? Use a calculator to do the arithmetic.)
</li>
<li>
Similarly, use a Taylor series to approximate <span class="math inline">\(\cos(62\degree)\)</span>. (Note that this is just <span class="math inline">\(\cos(\pi/3 + \pi/90)\)</span>, so do you think that you would get a better approximation using <span class="math inline">\(\cos(x)\)</span> expanded around <span class="math inline">\(0\)</span> or around <span class="math inline">\(\pi/3\)</span>?)
</li>
<li>
Estimate <span class="math inline">\(\sin(35\degree)\)</span>
</li>
<li>
Here’s a similar question: what’s <span class="math inline">\(e\)</span>? As in, what is its decimal approximation? Rather than just punching ``<span class="math inline">\(e\)</span>’’ into your calculator and having it tell you, estimate it yourself! Use a Taylor series to approximate it; write it out to at least a half-dozen terms and then find the decimal.
</li>
<li>
Can you approximate <span class="math inline">\(e^2\)</span> (to, say, three significant figures)?
</li>
<li>
Approximate <span class="math inline">\(\sqrt{38}\)</span> (again, to three or four decimal places).
</li>
<li>
Estimate <span class="math inline">\(\ln(1.4)\)</span>.
</li>
<li>
Use a Taylor series to estimate <span class="math inline">\(\sin(0)\)</span>. (Obviously, you know that this is <span class="math inline">\(0\)</span>—but I want you to see what the Taylor series tells you.) Do the same for <span class="math inline">\(\cos(0)\)</span>.
</li>
<li>
On a similar but somewhat more interesting note, try using a Taylor series to estimate <span class="math inline">\(\sin(\pi)\)</span>. Do the same for <span class="math inline">\(\cos(\pi)\)</span>. How close to the exact answers do you get after one term? two terms? three terms? four terms? ten terms?
</li>
</ol>
<div style="height:3em;">

</div>
More fun with Taylor series!
<ol class="problems">
<li>
It is a well-known trig identity that <span class="math inline">\(\cos(-x) = \cos(x)\)</span> (i.e., that a horizontal reflection of a cosine gives you just a cosine—that cosine is symmetric around the <span class="math inline">\(y\)</span>-axis). Prove this using a Taylor series. (You could prove it in other ways, but I want you to prove it using a T.S.)
</li>
<li>
Likewise, use a Taylor series to prove that <span class="math inline">\(-\sin(x) = \sin(-x)\)</span> (i.e., that a vertical reflection of sine is equivalent to a horizontal reflection).
</li>
</ol>
<div style="height:3em;">

</div>
Even more fun with Taylor series! Below are two limits which you can’t do just by plugging in <span class="math inline">\(0\)</span>, since you’d get a divide-by-zero error. Ordinarily our workaround would be to use L’Hopital’s rule. But we can do a Taylor series, too! Evaluate the following limits using both 1) L’Hopital’s rule and 2) a Taylor series.
<ol class="problems">
<span class="math inline">\(\displaystyle \lim_{x\rightarrow 0} \frac{1-\cos x}{x^2}\)</span>
<span class="math inline">\(\displaystyle \lim_{x\rightarrow 0} \frac{\sin(x) - x}{x^2}\)</span>
</ol>
<div style="height:3em;">

</div>
<ol class="problems">
<li>
A few weeks ago, when we were talking about series, I told you that it was far harder to figure out what a series converges to than to figure out whether it converges. Like with antiderivatives, all we have are a smattering of {}, particular formulas—there is no general method. So in this problem, I’m going to give you another equation for what a certain series converges to, and you’re going to prove why it’s true. Here’s the formula: <span class="math display">\[\sum_{n=1}^\infty \frac{n}{(n+1)!} = 1\]</span> It takes a little bit of work to do this. My suggestion is to find the Taylor series of <span class="math inline">\(\displaystyle f(x) = \frac{e^x-1}{x}\)</span>, take a derivative, and play with it a bit. Keep in mind that you have three ways to write this function—you can write it as <span class="math inline">\((e^x-1)/x\)</span>, you can write it as a written-out polynomial, and you can write it in <span class="math inline">\(\Sigma\)</span>-form. This is a cool proof, but don’t give up if you can’t do it in thirty seconds. Muse about it! (Also, don’t let it escape your notice that the formula starts with <span class="math inline">\(n=1\)</span>, whereas our Taylor series usually start with <span class="math inline">\(n=0\)</span>.)
</li>
<li>
Here’s another weird, unexpected, and counterintuitive series formula: <span class="math display">\[\sum_{n=1}^\infty \frac{1}{n!(n+2)} = \frac{1}{2}\]</span> I want you to prove this, too. Try finding the Taylor series of <span class="math inline">\(f(x) = xe^x\)</span>, taking an integral, and playing with it for a bit.
</li>
</ol>
<div style="height:3em;">

</div>
<ol class="problems">
<li>
Differentiate the Taylor series for <span class="math inline">\(\sin(x)\)</span>, <span class="math inline">\(\cos(x)\)</span>, and <span class="math inline">\(e^x\)</span> term-by-term, and compare your results with the Taylor series for <span class="math inline">\(\cos(x)\)</span>, <span class="math inline">\(-\sin(x)\)</span>, and <span class="math inline">\(e^x\)</span>, respectively.
</li>
<li>
Integrate the Taylor series for <span class="math inline">\(\sin(x)\)</span>, <span class="math inline">\(\cos(x)\)</span>, and <span class="math inline">\(e^x\)</span> term-by-term, and compare your results with the Taylor series for <span class="math inline">\(-\cos(x)\)</span>, <span class="math inline">\(\sin(x)\)</span>, and <span class="math inline">\(e^x\)</span>, respectively.
</li>
</ol>
<div style="height:3em;">

</div>
Making antiderivatives with Taylor series!
<ol class="problems">
<li>
Integrate <span class="math inline">\(\displaystyle\int \! \sin(x^2)\,dx\)</span>. Then estimate <span class="math inline">\(\displaystyle\int_0^1 \sin(x^2)\,dx\)</span>. (Do it to at least three significant figures—just keep adding up the terms until they settle down to at least three sig. figs.)
</li>
<li>
Likewise, estimate <span class="math inline">\(\displaystyle\int_0^{0.1} \! \frac{\sin x}{x}\,dx\)</span>
</li>
<li>
Estimate <span class="math inline">\(\displaystyle\int_0^{0.1} \sqrt{1+x^4}\,dx\)</span>
</li>
<li>
Estimate <span class="math inline">\(\displaystyle\int_0^{0.1} x^2e^{-x^2}\,dx\)</span>
</li>
<li>
Estimate <span class="math inline">\(\displaystyle\int_0^1 \! \frac{1-\cos x}{x^2}\,dx\)</span>
</li>
</ol>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>He’s really cool. 1861–1947.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>I hate using quotes without citation, but I don’t actually know what this is from. It might well be apocryphal.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>Google it! Read it!<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</main>
</body>
</html>