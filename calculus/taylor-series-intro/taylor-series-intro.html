<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Everything Is Polynomic: Adventures With Taylor Series</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="/main.css" />
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <script src="../highlight-toc.js" type="text/javascript"></script>
</head>
<body>
<script>document.addEventListener('DOMContentLoaded', function(e) {
  Scroller.init();
})</script>
<aside>
<div class="tocLinks" >
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#antidifferentiation-awesomeness">Antidifferentiation Awesomeness</a></li>
<li><a href="#whence-taylor-series-the-proof">Whence Taylor Series: The Proof</a></li>
</ul>
</nav>
</div>
</aside>
<main>
<header id="title-block-header">
<h1 class="title">Everything Is Polynomic: Adventures With Taylor Series</h1>
</header>
<p>The core assertion/paradox/weirdness/etc. behind calculus is:</p>
<p><span class="math display">\[\begin{array}{l}
\text{if we zoom in infinitely far, everything becomes straight}
\end{array}\]</span> Or, put differently: <span class="math display">\[\begin{array}{l}
\text{if we zoom in far enough, everything becomes straight}
\end{array}\]</span> Or: <span class="math display">\[\begin{array}{l}
\text{if we zoom in far enough, everything becomes a first-degree polynomial}
\end{array}\]</span> In a sense, Taylor’s theorem generalizes this paradox/assertion/conclusion/observation. It generalizes it from just about lines to about arbitrary-degree polynomials. Taylor’s theorem says: <span class="math display">\[\begin{array}{l}
\text{\small if we zoom in far enough, everything becomes a zeroth-degree polynomial} \\
\text{\small if we zoom in far enough, everything becomes a first-degree polynomial} \\
\text{\small if we zoom in far enough, everything becomes a quadratic polynomial} \\
\text{\small if we zoom in far enough, everything becomes a cubic polynomial} \\
\text{\small if we zoom in far enough, everything becomes a quartic polynomial} \\
\text{\small if we zoom in far enough, everything becomes a fifth-degree polynomial} \\
\text{\small if we zoom in far enough, everything becomes a sixth-degree polynomial} \\
\hfill \vdots \hfill
\end{array}\]</span> Or, put more simply: <span class="math display">\[
\left.
\begin{array}{r}
\text{\small if we zoom in far enough, everything becomes a zeroth-degree polynomial} \\
\text{\small if we zoom in far enough, everything becomes a first-degree polynomial} \\
\text{\small if we zoom in far enough, everything becomes a quadratic polynomial} \\
\text{\small if we zoom in far enough, everything becomes a cubic polynomial} \\
\text{\small if we zoom in far enough, everything becomes a quartic polynomial} \\
\text{\small if we zoom in far enough, everything becomes a fifth-degree polynomial} \\
\text{\small if we zoom in far enough, everything becomes a sixth-degree polynomial} \\
\hfill \vdots \hfill
\end{array}
\right\} \substack{\text{\large  if we zoom in far enough,}\\\text{\large  everything becomes an}\\\text{\large $n$th-degree polynomial}\\\text{\large  for any/all $n$}}
\]</span> I.e., just: <span class="math display">\[\begin{array}{l}
\text{if we zoom in far enough, everything becomes an $n$th-degree polynomial} \\
\hfill \text{for any/all $n$} \hfill
\end{array}\]</span></p>
<blockquote>
<p><em>We shall not cease from exploration,</em> <br> <em>And the end of all our exploring,</em> <br> <em>Will be to arrive where we started</em> <br> <em>And know the place for the first time.</em> <br> <span style="text-align:right;">—T.S. Eliot, <em>Little Gidding</em></span></p>
</blockquote>
<div style="height:5em">

</div>
<p>The English philosopher/mathematician Alfred North Whitehead<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> once remarked that “all of Western philosophy is a series of footnotes to Plato.”<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> My feelings about Taylor series are similar, but the chronology is reversed. In a sense, all of the math we’ve done up until now—all of the complicated proofs and convoluted arguments—we could have done much more simply, and much more cleanly, using Taylor series. If you glance at the problems in the back, you should get an idea of their wide-ranging versatility and utility. If the world is made only of polynomials, it really is a much simpler place.</p>
<p>I’ve built up lots of anticipation for Taylor series—too much, I worry, since I don’t want to make it seem anticlimactic—but we’re finally here. Let’s go.</p>
<div style="height:5em">

</div>
<p>We can turn ANY FUNCTION into an INFINITELY-LONG POLYNOMIAL, by using this formula: <span class="math display">\[\begin{align*}
\text{a polynomialized function} =&amp; \text{   (the value of the function, when $x=0$)}\\ \\
&amp;+(\text{the slope of the function, at $x=0$})\cdot x \\ \\
&amp;+\left(\frac{\text{the slope of the slope of the function, at $x=0$}}{2}\right)\cdot x^2  \\ \\
&amp;+\left(\frac{\text{the slope of the slope of the slope of the function, at $x=0$}}{6}\right)\cdot x^3 \\ \\
&amp;+\left(\frac{\substack{\text{the slope of the slope of}\\\text{the slope of the function}}\text{ at $x=0$}}{24}\right)\cdot x^4 \\ \\
&amp;\quad\vdots \\ \\
&amp;+\left(\frac{\overbrace{\text{the slope of the slope of the slope of }\cdots}^{n\text{ times}} \text{ the function}\text{, at }x=0}{n!}\right)\cdot x^n \\ \\
&amp;\quad\vdots \\ \\
&amp;\quad\text{ON AND ON INFINITELY!}
\end{align*}\]</span></p>
<p>If you want to write this with fancy big-sigma <span class="math inline">\(\Sigma\)</span> sum notation, you can write it all as:</p>
<p><span class="math display">\[\sum_{n=0}^{n=\infty}\frac{\overbrace{\text{the slope of the slope of the slope of }\cdots}^{n\text{ times}} \text{ the function}\text{, at }x=0}{n!}\,\cdot\, x^n\]</span></p>
<p>In this formula <span class="math inline">\(x=a\)</span> is some <span class="math inline">\(x\)</span>-value of the function. It’s kind of like we’re <em>growing</em> the polynomial around that point—the more and more terms we add, the more and more this longer and longer polynomial looks like the function.</p>
<p>So, in class, we saw that if we want to write sine as polynomial, we can write it like: <span class="math display">\[\sin x = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \frac{x^9}{9!} -\cdots\]</span> And we saw that the more and more terms we added on, it was like we were <em>growing</em> the polynomial! We were growing it outward from the origin!</p>
<p>If we wanted to grow it outwards from somewhere else, we could just shift it left or right. So, for example, if you wanted to turn sine into a polynomial, but grow it outwards from, say, <span class="math inline">\(3\pi\)</span>, you could write: <span class="math display">\[\sin x = (x-3\pi) - \frac{(x-3\pi)^3}{3!} + \frac{(x-3\pi)^5}{5!} - \frac{(x-3\pi)^7}{7!} + \frac{(x-3\pi)^9}{9!} -\cdots\]</span></p>
<p>The big formula—<strong>Taylor’s Formula</strong> or <strong>Taylor’s Theorem</strong> if you want to look it up—for polynomializing a function by growing it outwards from <span class="math inline">\(x=a\)</span> would then look like:</p>
<p><span class="math display">\[\sum_{n=0}^{n=\infty}\frac{\overbrace{\text{the slope of the slope of the slope of }\cdots}^{n\text{ times}} \text{ the function}\text{, at }x=a}{n!}\,\cdot\, (x-a)^n\]</span></p>
<p>By the way, it’s not strictly true that we can turn <em>any</em> function into a polynomial; there are some caveats. But that’s the big idea! <em>Everything is a polynomial</em>.</p>
<p>Given some function <span class="math inline">\(f(x)\)</span> (of which we can take derivatives and such), and some <span class="math inline">\(x\)</span>-value of the function <span class="math inline">\(a\)</span>, then we can write <span class="math inline">\(f(x)\)</span> as a (possibly-infinitely-long) polynomial in this way: <span class="math display">\[f(x) = f(a) + f&#39;(a)(x-a) + \frac{f&#39;&#39;(a)}{2!}(x-a)^2 + \frac{f&#39;&#39;&#39;(a)}{3!}(x-a)^3 + \frac{f&#39;&#39;&#39;&#39;(a)}{4!}(x-a)^4 + \cdots\]</span> or, written in <span class="math inline">\(\Sigma\)</span>-form: <span class="math display">\[f(x) = \sum_{k=0}^{\infty}\frac{f^{(k)}(a)}{k!}(x-a)^k\]</span> This is called the <strong>Taylor series</strong> or <strong>Taylor expansion</strong> of <span class="math inline">\(f(x)\)</span>. Sometimes it is also called the Taylor series/expansion <strong>around <span class="math inline">\(a\)</span></strong> or <strong>in terms of <span class="math inline">\(x-a\)</span></strong>. Taylor series where <span class="math inline">\(a=0\)</span> (which is a common choice for <span class="math inline">\(a\)</span>, since it tends to make the algebra easier) are often called <strong>Maclaurin series</strong>. If we approximate a function using a Taylor series of finite length—i.e., if we have a function that should be an infinitely-long polynomial, but we approximate it with a finitely-long one—we call that the <strong>Taylor approximation</strong> of <span class="math inline">\(f(x)\)</span>, or the <strong>Taylor polynomial of order <span class="math inline">\(n\)</span></strong>, where <span class="math inline">\(n\)</span> is the number of terms in the Taylor series we’ve written out.</p>
<p>(Note that in these notes, when I write something like <span class="math inline">\(f^{(2)}(x)\)</span> (or <span class="math inline">\(f^{(n)}(x)\)</span>), I mean the second (or <span class="math inline">\(n\)</span>th) derivative of <span class="math inline">\(f(x)\)</span>.)</p>
<p>Let’s use this to try writing <span class="math inline">\(\sin x\)</span> as a polynomial. For <span class="math inline">\(a\)</span>, let’s choose <span class="math inline">\(a=0\)</span>. (The choice of <span class="math inline">\(a\)</span> doesn’t matter; it could be any number. I’m choosing <span class="math inline">\(0\)</span> because we’ll have to plug <span class="math inline">\(a\)</span> into the various derivatives of <span class="math inline">\(\sin x\)</span>—i.e., we’ll have to plug <span class="math inline">\(a\)</span> into $x $ and <span class="math inline">\(\cos x\)</span>—and it’s a lot easier to find <span class="math inline">\(\sin(0)\)</span> and <span class="math inline">\(\cos(0)\)</span> than it is to find, say, <span class="math inline">\(\sin(43\pi/13.2)\)</span> (were I to choose <span class="math inline">\(a=43\pi/13.2\)</span>.) The formula is a tad messy, so to make things a bit clearer, let’s build it term-by-term. We’ve got a lot of derivatives to take and plug things into and compile, so I’m going to show things step-by-step in a table. In this table, I’ve listed the term index (on the left-hand column), the relevant derivative, the derivative with <span class="math inline">\(a\)</span> plugged in, and finally, the term itself.</p>
<p>So then, if I just add together all the terms in the rightmost column, I’ll get the Taylor series for <span class="math inline">\(\sin x\)</span>! I’ll have: <span class="math display">\[\sin(x) = 0 + x+0- \frac{1}{3!}x^3 +0 + \frac{1}{5!}x^5 + 0 -\frac{1}{7!}x^7 + 0 +\frac{1}{9!}x^9 - \cdots\]</span> or just: <span class="math display">\[\sin(x) = x- \frac{1}{3!}x^3 + \frac{1}{5!}x^5 -\frac{1}{7!}x^7 +\frac{1}{9!}x^9 - \cdots\]</span> There it is! We can write <span class="math inline">\(\sin(x)\)</span> as this infinitely-long polynomial, with alternating adding and substracting odd polynomials. If we like, we can represent in <span class="math inline">\(\Sigma\)</span>-form: <span class="math display">\[\sin(x) = \sum_{k=0}^{\infty} \frac{(-1)^{n+1}}{(2n+1)!}x^{2n+1}\]</span></p>
<p></p>
What if I want to write the Taylor series of, say, <span class="math inline">\(x^{20}\sin(x)\)</span>? The obvious way to do it would be to plug things into Taylor’s formula. But that will get real nasty real fast, since we’ll have to do the product rule (to find the first derivative), and then to find the second derivative we’ll have to do the product rule twice (because we get two terms), and then for the third derivative we’ll have to do the product rule four times, and so on. Once we’re past the 20th derivative the <span class="math inline">\(x^{20}\)</span> remnants will start to go away and we’ll be fine, but until then, it’ll be a huge mess.
<p>Why go to all that work? We know how to write <span class="math inline">\(\sin(x)\)</span> as a series—why don’t we just multiply the whole thing by <span class="math inline">\(x^{20}\)</span>? <span class="math display">\[\begin{align*}
x^{20}\sin(x) &amp;=x^{20}\cdot \left(x - \frac{1}{3!}x^3 + \frac{1}{5!}x^5 - \frac{1}{7!}x^7 + \frac{1}{9!}x^9 + \cdots \right) \\
 &amp;= x^{21} - \frac{1}{3!}x^{23} + \frac{1}{5!}x^{25} - \frac{1}{7!}x^{27} + \frac{1}{9!}x^{29} + \cdots \\
&amp;= \sum_{k=0}^{\infty} \frac{(-1)^{n+1}}{(2n+1)!}x^{2n+21} 
\end{align*}\]</span></p>
<p>Likewise, what if you want to find the Taylor series of <span class="math inline">\(\sin(5x)\)</span>? Again, you {} brute-force it—you could plug this into Taylor’s formula. And, actually, it’d be a lot easier than in the previous example—you’d just get a bunch of <span class="math inline">\(5\)</span>’s piling up as a result of the chain rule. But still. Here’s my idea: you already know the Taylor series of <span class="math inline">\(\sin(x)\)</span>. So why not just replace <span class="math inline">\(x\)</span> with <span class="math inline">\(5x\)</span>? We’d have: <span class="math display">\[\begin{align*}
\sin(5x) &amp;= 5x - \frac{1}{3!}(5x)^3 + \frac{1}{5!}(5x)^5 - \frac{1}{7!}(5x)^7 + \frac{1}{9!}(5x)^9 + \cdots \\
&amp;= 5x - \frac{5^3}{3!}x^3 + \frac{5^5}{5!}x^5 - \frac{5^7}{7!}x^7 + \frac{5^9}{9!}x^9 + \cdots \\  
&amp;= \sum_{k=0}^{\infty} \frac{(-1)^{n+1}5^{2n+1}}{(2n+1)!}x^{2n+1} 
\end{align*}\]</span></p>
<h2 id="antidifferentiation-awesomeness">Antidifferentiation Awesomeness</h2>
<p>Here’s a cool application of Taylor series. So, antiderivatives are a pain, right? And sometimes they’re not just hard; they’re {}. Like, literally, you can prove that it’s impossible to antidifferentiate some functions. Classic examples: <span class="math display">\[\int \! \frac{\sin x}{x} \,dx \quad\text{ and }\quad \int \!  \sqrt{1+x^4}\,dx\]</span> But really… these are only impossible to antidifferentiate {}. Here’s what I mean: We know we can write sine using a Taylor series: <span class="math display">\[\sin(x) = x - \frac{1}{3!}x^3 + \frac{1}{5!}x^5 - \frac{1}{7!}x^7 + \frac{1}{9!}x^9 + \cdots\]</span> So then we should be able to write <span class="math inline">\(\sin(x)/x\)</span> like this: <span class="math display">\[\begin{align*}
\frac{\sin(x)}{x} &amp;= \left(\frac{1}{x}\right)\sin(x) \\
 &amp;= \left(\frac{1}{x}\right)\cdot\left(x - \frac{1}{3!}x^3 + \frac{1}{5!}x^5 - \frac{1}{7!}x^7 + \frac{1}{9!}x^9 + \cdots \right) \\
 \intertext{}
 &amp;= 1 - \frac{1}{3!}x^2 + \frac{1}{5!}x^4 - \frac{1}{7!}x^6 + \frac{1}{9!}x^8 + \cdots \\
&amp;= \sum_{k=0}^{\infty} \frac{(-1)^{n+1}}{(2n+1)!}x^{2n} 
\end{align*}\]</span> But wait… this is just a polynomial. We can antidifferentiate polynomails. That’s {}. We must just have: <span class="math display">\[\begin{align*}
\int \! \frac{\sin x}{x}\,dx &amp;= \int \! 1 - \frac{1}{3!}x^2 + \frac{1}{5!}x^4 - \frac{1}{7!}x^6 + \frac{1}{9!}x^8 +\cdots \,dx \\
&amp;= x - \frac{1}{3!\cdot 3}x^3 + \frac{1}{5!\cdot5}x^5 - \frac{1}{7!\cdot 7}x^7 + \frac{1}{9!\cdot 9}x^9 +\cdots + C \\
&amp;= \sum_{k=0}^{\infty} \frac{(-1)^{n+1}}{(2n+1)!\cdot(2n+1)}x^{2n+1} 
\end{align*}\]</span> WOW!!!!! We can take an antiderivative of this supposedly-impossible-to-antidifferentiate function. That’s really, really cool.</p>
<h2 id="whence-taylor-series-the-proof">Whence Taylor Series: The Proof</h2>
<p>We should probably understand why Taylor series exist before we go much further. The basic idea is that a Taylor series is really just a tricked-out version of integration by parts. When we were doing integration by parts, we discussed how to quickly do multiple integrations-by-parts (in the case of, e.g., <span class="math inline">\(\int x^5\sin(x)dx\)</span>), and I probably made some analogy to the effect of, ``If integration by parts is a machine gun, using a table is an {}!!!’’ Taylor series continues this analogy: it makes integrating by parts or using a table seem like small-arms fire. Taylor series, by comparison, is an {}.</p>
<p>Imagine we have some function <span class="math inline">\(h(x)\)</span> and some point <span class="math inline">\(a\)</span>. Then, as we’ve discussed, we can write it as: <span class="math display">\[h(x)= h(a) + (x-a)h&#39;(a)  + \frac{1}{2!}(x-a)^2h&#39;&#39;(a) + \frac{1}{3!}(x-a)^3h&#39;&#39;&#39;(a)  +\frac{1}{4!}(x-a)^4h&#39;&#39;&#39;&#39;(a) + \cdots\]</span> (Note that I’m calling this function <span class="math inline">\(h(x)\)</span> and not <span class="math inline">\(f(x)\)</span>; that’s because in the proof I want to use the integration-by-parts formula, which already has a function labelled as <span class="math inline">\(f(x)\)</span>, and I don’t want to get it confused. Also, I wrote it in a slightly-switched around order, but multiplication doesn’t care about order (<span class="math inline">\(ab=ba\)</span>), so that’s no big deal.) Let’s prove this.</p>
<p>Where should we start? How about at the beginning—with the fundamentals? Part II of the Fundamental Theorem of Calculus says that for any function <span class="math inline">\(h(x)\)</span>, we must have: <span class="math display">\[\int_a^x \! h&#39;(t) \, dt = h(x) - h(a)\]</span> Of course, I can rearrange this so that it just looks like: <span class="math display">\[h(x) = h(a) + \int_a^x \! h&#39;(t) \, dt\]</span> And we’re on our way! We’ve already started creating a Taylor polynomial—<span class="math inline">\(h(a)\)</span> is the first term. Now I want to start hitting up <span class="math inline">\(\int h&#39;(t)dt\)</span> with integration by parts. Remember that the integration by parts formula looks like this: <span class="math display">\[\int\! f(x)g&#39;(x)\,dx = f(x)g(x) - \int\! f&#39;(x)g(x)\,dx\]</span> Of course, in our Taylor formula so far, we have an integral that only has one part. It only has <span class="math inline">\(h&#39;(t)\)</span>. But really it has {} parts—because it’s <span class="math inline">\(h&#39;(t)\)</span> multiplied by <span class="math inline">\(1\)</span>: <span class="math display">\[h(x) = h(a) + \int_a^x \! 1\cdot h&#39;(t) \, dt\]</span> So let’s choose <span class="math inline">\(1\)</span> to be our <span class="math inline">\(g&#39;(x)\)</span> (or, well, <span class="math inline">\(g&#39;(t)\)</span>), and <span class="math inline">\(h&#39;(t)\)</span> to be our <span class="math inline">\(f(t)\)</span>: <span class="math display">\[h(x) = h(a) + \int_a^x \! \underbrace{1}_{\mathclap{g&#39;(t)}}\cdot \underbrace{h&#39;(t)}_{f(t)} \, dt\]</span> And then we can list the various stuff we need to integrate by parts. (One of these is kind of weird and maybe surprising. I’ll explain in a moment.)</p>
<p>The <span class="math inline">\(f\)</span> and <span class="math inline">\(f&#39;\)</span> should make sense—<span class="math inline">\(f\)</span> is <span class="math inline">\(h&#39;\)</span>, so <span class="math inline">\(f&#39;\)</span> is <span class="math inline">\(h&#39;&#39;\)</span>. But what you should be immediately suspicious of is my choice of <span class="math inline">\(g(t)\)</span>. <span class="math inline">\(g&#39;(t)\)</span> was <span class="math inline">\(1\)</span>, so shouldn’t <span class="math inline">\(g(t)\)</span>—which is just the antiderivative of <span class="math inline">\(g&#39;(t)\)</span>—be <span class="math inline">\(t\)</span>?</p>
Yes and no. Allow me, first, to prove to you that, in fact, <span class="math inline">\(-(x-t)\)</span> {} an antiderivative of <span class="math inline">\(1\)</span>. The key step here is realizing that we’re taking the derivative/antiderivative {}, and that <span class="math inline">\(x\)</span> is {} with respect to <span class="math inline">\(t\)</span>. If I take the derivative of <span class="math inline">\(-(x-t)\)</span>, I get: <span class="math display">\[\begin{align*}
\frac{d}{dt}\left[ -(x-t)\right] &amp;= \frac{d}{dt}\left[ -x + t \right] \\
&amp; = \frac{d}{dt}\left[ -x \right]  + \frac{d}{dt}\left[t \right] \\
&amp;= 0 + 1 \\
&amp;= 1
\end{align*}\]</span> Does this make sense? The derivative of <span class="math inline">\(-(x-t)\)</span> is <span class="math inline">\(1\)</span>; therefore, the antiderivative of <span class="math inline">\(1\)</span> is <span class="math inline">\(-(x-t)\)</span>. So if I put this stuff together and integrate by parts, I get: <span class="math display">\[h(x) = h(a) + \left[-(x-t)h&#39;(t) - \int\! -(x-t)h&#39;&#39;(t)\,dt \right]_a^x\]</span> <span class="math display">\[h(x) = h(a) + \left[-(x-t)h&#39;(t)\right]_a^x + \left[ - \int\! -(x-t)h&#39;&#39;(t)\,dt \right]_a^x\]</span> And then if I evaluate that (i.e., plug in the <span class="math inline">\(x\)</span> and <span class="math inline">\(a\)</span>): <span class="math display">\[h(x) = h(a) + (-(x-x)h&#39;(x) - [-(x-a)h&#39;(a)] ) - \int_a^x\! -(x-t)h&#39;&#39;(t)\,dt \]</span> We’ve got a whole bunch of nested negatives—let’s distribute them and clean this up. <span class="math display">\[h(x) = h(a) -(\underbrace{x-x}_{=0})h&#39;(x)+ (x-a)h&#39;(a)  + \int_a^x\! (x-t)h&#39;&#39;(t)\,dt \]</span> Also, that second term has <span class="math inline">\((x-x)\)</span>, which is just zero, and so the entire term goes away: <span class="math display">\[h(x) = h(a) + 0+ (x-a)h&#39;(a)  + \int_a^x\! (x-t)h&#39;&#39;(t)\,dt \]</span> <span class="math display">\[h(x) = h(a) + (x-a)h&#39;(a)  + \int_a^x\! (x-t)h&#39;&#39;(t)\,dt \]</span> It looks like the first two terms of a Taylor series! Let’s keep going! Let’s integrate THIS by parts! <span class="math display">\[h(x) = h(a) + (x-a)h&#39;(a)  + \int_a^x\! \underbrace{(x-t)}_{g&#39;(t)}\cdot\underbrace{h&#39;&#39;(t)}_{f(t)}\,dt \]</span>
so we have: <span class="math display">\[\begin{align*}
h(x) &amp; = h(a) + (x-a)h&#39;(a)  + \left[ -\frac{1}{2}(x-t)^2h&#39;&#39;(t) - \int\! -\frac{1}{2}(x-t)^2h&#39;&#39;&#39;(t) \,dt   \right]_a^x \\ \\
&amp;= h(a) + (x-a)h&#39;(a)  + \left( -\frac{1}{2}(x-x)^2h&#39;&#39;(x) - (-\frac{1}{2}(x-a)^2h&#39;&#39;(a))\right) - \int_a^x\! -\frac{1}{2}(x-t)^2h&#39;&#39;&#39;(t) \,dt   \\ \\ 
&amp; = h(a) + (x-a)h&#39;(a)  + \left( -\frac{1}{2}(\underbrace{x-x}_{=0})^2h&#39;&#39;(x) - (-\frac{1}{2}(x-a)^2h&#39;&#39;(a))\right) - \int_a^x\! -\frac{1}{2}(x-t)^2h&#39;&#39;&#39;(t) \,dt   \\ \\
&amp; = h(a) + (x-a)h&#39;(a)  + \left( 0 - (-\frac{1}{2}(x-a)^2h&#39;&#39;(a))\right) - \int_a^x\! -\frac{1}{2}(x-t)^2h&#39;&#39;&#39;(t) \,dt   \\ \\
&amp;= h(a) + (x-a)h&#39;(a)  + \frac{1}{2}(x-a)^2h&#39;&#39;(a) - \int_a^x\! -\frac{1}{2}(x-t)^2h&#39;&#39;&#39;(t) \,dt   \\ \\
&amp;= h(a) + (x-a)h&#39;(a)  + \frac{1}{2}(x-a)^2h&#39;&#39;(a) + \int_a^x\! \frac{1}{2}(x-t)^2h&#39;&#39;&#39;(t) \,dt 
\end{align*}\]</span> I guess one of the annoying things about this proof is all the nested negatives—negatives of negatives of negatives. So be sure you keep track of those. Anyway, roll on! Let’s integrate the NEXT integral by parts!!! <span class="math display">\[h(x)= h(a) + (x-a)h&#39;(a)  + \frac{1}{2}(x-a)^2h&#39;&#39;(a) + \int_a^x\! \underbrace{\frac{1}{2}(x-t)^2}_{g&#39;(t)}\cdot\underbrace{h&#39;&#39;&#39;(t)}_{f(t)} \,dt \]</span>
<p><span class="math display">\[h(x)= h(a) + (x-a)h&#39;(a)  + \frac{1}{2}(x-a)^2h&#39;&#39;(a) +\left[ -\frac{1}{2\cdot3}(x-t)^3h&#39;&#39;&#39;(t) -  \int \! -\frac{1}{2\cdot3}(x-t)^3f&#39;(t) = h&#39;&#39;&#39;&#39;(t) \right]_a^x \]</span> I’m sort of running out of space on the page, but you get the idea: I plug in <span class="math inline">\(x\)</span> and <span class="math inline">\(a\)</span>, I get an <span class="math inline">\((x-x)\)</span> term that goes away, I blast through a bunch of negatives, and end up with something like: <span class="math display">\[h(x)= h(a) + (x-a)h&#39;(a)  + \frac{1}{2}(x-a)^2h&#39;&#39;(a) + \frac{1}{2\cdot3}(x-a)^3h&#39;&#39;&#39;(a) -  \int_a^x \! -\frac{1}{2\cdot3}(x-t)^3h&#39;&#39;&#39;&#39;(t)\,dt \]</span> So, as you can see, a Taylor polynomial is forming! We are growing a Taylor series, bit by bit. Instead of giving it sun and water, we are giving it integration by parts. All we do is integrate by parts forever, and we get a Taylor series! So this shows you where the formula comes from, which as far as I am concerned is sufficient proof. </p>
{
<p>}</p>
Use Taylor’s formula (the formula) to find the Taylor series of the following functions. (Write them out to at least the 5th nonzero term and also express them in <span class="math inline">\(\Sigma\)</span> form. If I don’t give you a value for <span class="math inline">\(a\)</span>, choose one yourself.)
Find the Taylor series of each of the following rational functions:
<p>In 1974, the philosopher Thomas Nagel published a seminal essay on the philosophy of mind entitled <code>What is it like to be a bat?''\footnote{Google it! Read it!} His thesis, simply put, was that we cannot know what it is like to be a bat, because if we did, we would either be a strange bat-human hybrid (and thus not really a bat) or a bat (and thus we wouldn't be ourselves). In the following problems, I would like you to ask yourselves:</code>What is it like to be a calculator?’’</p>
More fun with Taylor series!
Even more fun with Taylor series! Below are two limits which you can’t do just by plugging in <span class="math inline">\(0\)</span>, since you’d get a divide-by-zero error. Ordinarily our workaround would be to use L’Hopital’s rule. But we can do a Taylor series, too! Evaluate the following limits using both 1) L’Hopital’s rule and 2) a Taylor series.
Making antiderivatives with Taylor series!
<p>\end{document}</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>He’s really cool. 1861–1947.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>I hate using quotes without citation, but I don’t actually know what this is from. It might well be apocryphal.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</main>
</body>
</html>